{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGVzKonqk5YJ",
        "outputId": "6c667f04-d250-4a82-dea6-6eec6a65a4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1->torchtext) (2.2.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1->torchtext) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1->torchtext) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt372Ve6qwNt",
        "outputId": "21ca4854-8617-4457-8769-08070e6da270"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import string as st\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopword')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQZHbSGYlGLv",
        "outputId": "1697b918-dc01-4951-f9dc-86f4de3c8540"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Error loading stopword: Package 'stopword' not found in\n",
            "[nltk_data]     index\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext import data\n",
        "from torchtext import vocab\n"
      ],
      "metadata": {
        "id": "QI3xPoKZlH3O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_path_text=\"/content/drive/MyDrive/yelp_2013_texts.txt\"\n",
        "yelp_path_score=\"/content/drive/MyDrive/yelp_2013_score.txt\"\n",
        "texts = []\n",
        "scores = []\n",
        "\n",
        "texts = []\n",
        "scores = []\n",
        "with open(yelp_path_text, 'r', encoding='utf-8', errors=\"ignore\") as file:\n",
        "    for line in file:\n",
        "        texts.append(line.strip())\n",
        "with open(yelp_path_score, 'r') as file:\n",
        "    for line in file:\n",
        "        scores.append(line.strip())"
      ],
      "metadata": {
        "id": "Re1rWN27lKZV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_str(string):\n",
        "    string = re.sub(r'[^\\x00-\\x7F]+', r'', string)\n",
        "    string = re.sub(r\"\\\\\", \"\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string)\n",
        "    string = re.sub(r\"\\\"\", \"\", string)\n",
        "    string = re.sub(r\"<sssss>\", \"\", string)\n",
        "    string = re.sub(r\"-lrb-\", \"\", string)\n",
        "    string = re.sub(r\"-rrb-\", \"\", string)\n",
        "    string = re.sub(r\"\\.\\.\\.\", \"\", string)\n",
        "    string = string.strip().lower()\n",
        "    return string\n",
        "\n",
        "paired = list(zip(texts, scores))\n",
        "np.random.shuffle(paired)\n",
        "texts, scores = zip(*paired)\n",
        "texts = list(texts)\n",
        "scores = list(scores)\n"
      ],
      "metadata": {
        "id": "tvYedTcUlM6o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = texts[:5000]\n",
        "scores = scores[:5000]\n",
        "scores = [int(score) -1  for score in scores]"
      ],
      "metadata": {
        "id": "MGU4Or9PlQua"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_words = set()\n",
        "\n",
        "for string in texts:\n",
        "    words = string.split()\n",
        "    unique_words.update(words)\n",
        "\n",
        "unique_words = list(unique_words)\n",
        "print(\"Corpus size:\", len(unique_words))\n",
        "\n",
        "embedding_dim = 100\n",
        "global_vectors = GloVe(name='6B', dim=embedding_dim) # 42B, 840B\n",
        "\n",
        "corpus_size = len(unique_words)\n",
        "weights_matrix = np.zeros((corpus_size, embedding_dim))\n",
        "\n",
        "found_word = 0\n",
        "for i, word in enumerate(unique_words):\n",
        "  word_vector = global_vectors.get_vecs_by_tokens(word)\n",
        "\n",
        "  if word_vector.sum().item() == '0':\n",
        "    weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n",
        "  else:\n",
        "    weights_matrix[i] = word_vector\n",
        "    found_word += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40PTQZQblWLJ",
        "outputId": "1720a4bd-1ae3-4917-948e-a7f7b7647bb4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus size: 22548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y, unique_words, weights_matrix):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.unique_words = unique_words\n",
        "        self.weights_matrix = weights_matrix\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.X[idx]\n",
        "        label = self.y[idx]\n",
        "\n",
        "        indices = [self.unique_words.index(word) for word in sentence.split()]\n",
        "\n",
        "        return {\n",
        "            'input': torch.tensor(indices, dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "def collate_fn(batch):\n",
        "    inputs = [item['input'] for item in batch]\n",
        "    labels = [item['label'] for item in batch]\n",
        "\n",
        "    inputs_padded = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        'input': inputs_padded,\n",
        "        'label': torch.stack(labels)\n",
        "    }"
      ],
      "metadata": {
        "id": "dJtkbQxqlXD3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordAttention(nn.Module):\n",
        "  def __init__(self,hidden_size, word_embedd_dim) :\n",
        "    super(WordAttention, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.word_embedd_dim= word_embedd_dim\n",
        "    self.lin1 = nn.Linear(hidden_size,hidden_size)\n",
        "    self.lin2 =nn.Linear(hidden_size,1,bias = False)\n",
        "  def forward(self,x):\n",
        "    u = torch.tanh(self.lin1(x))\n",
        "    attention = F.softmax(self.lin2(x),dim=1)\n",
        "\n",
        "    output = torch.sum(attention*x, dim =1)\n",
        "\n",
        "    return attention, output\n",
        "\n",
        "class SenAttention(nn.Module):\n",
        "  def __init__(self, hidden_size, embedding_dim):\n",
        "    super(SenAttention, self).__init__()\n",
        "    self.lin1=nn.Linear(hidden_size,hidden_size)\n",
        "    self.lin2 = nn.Linear(hidden_size,1,bias=False)\n",
        "  def forward(self, x):\n",
        "    u = torch.tanh(self.lin1(x))\n",
        "    attention = F.softmax(self.lin2(u),dim =1)\n",
        "\n",
        "    output = torch.sum(attention * x, dim =1)\n",
        "\n",
        "    return attention, output\n",
        "\n",
        "class WordEncoder(nn.Module):\n",
        "  def __init__(self, corpus_size, embedding_dim, hidden_size, load_embed=False, weights_matrix=None, trainable_embedding= False):\n",
        "    super(WordEncoder, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(corpus_size, embedding_dim)\n",
        "\n",
        "    if load_embed and weights_matrix is not None :\n",
        "      self.embedding.load_state_dict({'weight': torch.tensor(weights_matrix)})\n",
        "\n",
        "    self.embedding.weight.requires_grad = trainable_embedding\n",
        "    self.gru = nn.GRU(embedding_dim, hidden_size,2, dropout=0.3, bidirectional= True,batch_first = True)\n",
        "    self.attention = WordAttention(hidden_size * 2, embedding_dim)\n",
        "  def forward(self, x):\n",
        "    embeddings = self.embedding(x)\n",
        "    out, hidden = self.gru(embeddings)\n",
        "    attention, out = self.attention(out)\n",
        "    return out\n",
        "\n",
        "class HAN(nn.Module):\n",
        "  def __init__(self, corpus_size, embedding_dim, hidden_size,load_embed= False, weights_matrix=None,trainable_embedding=False):\n",
        "    super(HAN,self).__init__()\n",
        "\n",
        "    self.wordEncoder=WordEncoder(corpus_size=corpus_size,embedding_dim=embedding_dim,hidden_size=50, load_embed=True, weights_matrix=weights_matrix, trainable_embedding=True)\n",
        "\n",
        "    self.sentGRU= nn.GRU(hidden_size*2, hidden_size, bidirectional=True, batch_first=True)\n",
        "    self.sentence_att = SenAttention(hidden_size * 2, hidden_size)\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.classifer= nn.Linear(hidden_size*2,5)\n",
        "  def forward(self, x):\n",
        "    word_output= self.wordEncoder(x)\n",
        "\n",
        "    sen_out,_ = self.sentGRU(word_output.unsqueeze(1))\n",
        "    _, sen_output = self.sentence_att(sen_out)\n",
        "\n",
        "    out = self.classifer(sen_output)\n",
        "    return F.softmax(out,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DdGGZ0EwldQ7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tranning**"
      ],
      "metadata": {
        "id": "ECjoqAaAlpk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "dataset = CustomDataset(X=texts,y=scores,unique_words = unique_words, weights_matrix = weights_matrix)\n",
        "train_size = int(0.8*len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset,[train_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size = len(test_dataset), shuffle = False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "8lZfLjn6le25"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optim, loss_fn, epochs=50, print_loss=True):\n",
        "  for epoch in range(50):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for i in train_loader:\n",
        "      optim.zero_grad()\n",
        "\n",
        "      output = model(i[\"input\"])\n",
        "      target = torch.tensor(i[\"label\"], dtype=torch.long)\n",
        "      target = target.unsqueeze(1)\n",
        "\n",
        "      loss = loss_fn(output, target.squeeze())\n",
        "      epoch_loss += loss.item()\n",
        "\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        eval_loss = 0\n",
        "        for i in test_loader:\n",
        "            output = model(i[\"input\"])\n",
        "            target = torch.tensor(i[\"label\"], dtype=torch.long)\n",
        "\n",
        "            loss = loss_fn(output, target)\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "    if print_loss:\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"Epoch loss:\", round(epoch_loss / len(train_loader), 4))\n",
        "            print(\"Eval Loss:\", round(eval_loss / len(test_loader), 4))\n",
        "\n",
        "    print(\"Eval Loss:\", round(eval_loss / len(test_loader), 4))\n",
        "    return model"
      ],
      "metadata": {
        "id": "G0aI8uJwoiVX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_size, embedding_dim = weights_matrix.shape\n",
        "han_model = HAN(corpus_size=corpus_size, embedding_dim=embedding_dim, hidden_size=50, load_embed=True, weights_matrix=weights_matrix, trainable_embedding=False)\n",
        "optim =  torch.optim.Adam(han_model.parameters(), 0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "han_model = train(han_model, optim, loss_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMuPVzhap_F4",
        "outputId": "9c3b4aea-a37f-4851-e300-c08236ff7581"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-bc379bc3b118>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(i[\"label\"], dtype=torch.long)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch loss: 1.5074\n",
            "Eval Loss: 1.4994\n",
            "Eval Loss: 1.4994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-bc379bc3b118>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  target = torch.tensor(i[\"label\"], dtype=torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "  for i in test_loader:\n",
        "      output = model(i[\"input\"])\n",
        "      target = i[\"label\"]\n",
        "\n",
        "\n",
        "  from sklearn.metrics import accuracy_score\n",
        "\n",
        "  print(accuracy_score(\n",
        "      target,\n",
        "      torch.argmax(output, dim=1)\n",
        "  ))\n",
        "\n",
        "evaluate(han_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb8lRnI6qeYa",
        "outputId": "1fc66abe-df21-4b85-a914-b4cabd5f27e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.352\n"
          ]
        }
      ]
    }
  ]
}